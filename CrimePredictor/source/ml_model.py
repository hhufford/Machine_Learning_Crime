# -*- coding: utf-8 -*-
"""FinalProject (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yxZQ5MtdsEtjdVio8DCZyDQrL6aH_ipL

# CSCI: 470 Final Project: 
## Predicting rates of violent crime given socio-economic data from communities in the United States
### Team H2Li: Heidi Hufford, Hannah Levy, and Lindsey Nield
---
**This code reads in the UCI Crime and Communities dataset (from a file stored on the computer) and does the following:**
- Splits the data into two datasets: a "community" dataset and a "police" dataset. This was done because it became obvious that this single dataset was made up of two different datasets, with differing numbers of observations
- Runs multiple linear regression on the community and police data to predict violent crime rate
- Runs principal component regression on the community and police data to predict violent crime rate. This was done because of the large number of features in each dataset.
- Runs lasso regression on the community and police data to predict violent crime rate. This is a shirnkage method that performs both variable selection and regularization. Again, it was used because of the large number of variables in the data sets.
- Converts continuous target to discrete target. The violent crime rates were classified as "low", "medium", or "high".
- Runs K-nearest neighbors algorithm on discrete target community and police data to predict level of violent crime
- Uses a feed-forward neural network to predict both rates and level of violent crime (discrete and continuous targets) for community and police data
-Uses feature importance procedures to determine feature importances of community data
-Uses top feature importances to construct "final" feed forward neural network

Note that there are two areas in the code where the dataset will need to uploaded. The first place is in the beginning, where the data is used for machine learning and predictive purposes. The second area is for the feature importance section; this was done to make ensure that the data being used for feature importance was not changed during the training and testing section.

## Read in Data
"""

# Commented out IPython magic to ensure Python compatibility.
#Set up code
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import sklearn as sk

from pandas import Series, DataFrame
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error
from sklearn import metrics

from sklearn.decomposition import PCA, KernelPCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_circles
from sklearn.preprocessing import MinMaxScaler

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# %matplotlib inline
plt.style.use("ggplot")

#Read in raw data
#Must have "communites.csv" on local drive
#Data can be downloaded from (http://archive.ics.uci.edu/ml/datasets/communities+and+crime) and saved as a csv 

from google.colab import files
uploaded = files.upload()

import io
raw_data = pd.read_csv(io.BytesIO(uploaded['communities.csv']))

#Ensure data has been imported correctly
raw_data.head()

"""## Data Preprocessing"""

#Replace '?' with NA values
raw_data = raw_data.replace('?', np.NaN)

#Find out more about our data
raw_data.describe(include="all")

#Check how many values in each column are null
raw_data.isna().sum()

#Remove fold column
raw_data = raw_data.drop(columns=["fold"])

#Remove the state, county, community, and community name columns
#Create dataframe of community statistics (1993 observations)
community_data = raw_data.copy()
community_data = community_data.drop(columns=["state", "county", "community", "communityname"])

community_data = community_data[[c for c in community_data if community_data[c].isnull().sum() < 1000]]
community_data.dropna(inplace=True)

#Create dataframe of crime/police statistics (319 observations)
police_data = raw_data[['LemasSwornFT',	'LemasSwFTPerPop',	'LemasSwFTFieldOps',	'LemasSwFTFieldPerPop',	'LemasTotalReq',	'LemasTotReqPerPop',	'PolicReqPerOffic',	'PolicPerPop',	'RacialMatchCommPol',	'PctPolicWhite',	'PctPolicBlack',	'PctPolicHisp',	'PctPolicAsian',	'PctPolicMinor',	'OfficAssgnDrugUnits',	'NumKindsDrugsSeiz',	'PolicAveOTWorked',	'PolicCars',	'PolicOperBudg',	'LemasPctPolicOnPatr',	'LemasGangUnitDeploy',	'PolicBudgPerPop', 'ViolentCrimesPerPop']].copy()
police_data.dropna(inplace=True)

community_data.head()

police_data.head()

"""#Regression Analysis

## Multiple Regression Model

### Using Community Data:
"""

#Turn dataframe into a matrix
X_community = community_data.copy()
X_community = X_community.drop(['ViolentCrimesPerPop'], axis=1)
X_community = X_community.values

y_community = community_data['ViolentCrimesPerPop'].copy()
y_community = y_community.values


#Split the data
X_train_community, X_test_community, y_train_community, y_test_community = train_test_split(X_community, y_community, random_state=0, test_size=0.2)
#X_model_community, X_valid_community, y_model_community, y_valid_community = train_test_split(X_train_community, y_train_community, random_state=0, test_size=0.2)

print(f"All Data:        {len(X_community)} points")
print(f"Training data:   {len(X_train_community)} points")
print(f"Testing data:    {len(X_test_community)} points")

#Create regression object
regressor_community = LinearRegression()  

#Fit it to our training data
regressor_community.fit(X_train_community, y_train_community)

#Used fitted model to predict values
y_pred_community = regressor_community.predict(X_test_community)

#Error metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_community, y_pred_community))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test_community, y_pred_community))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_community, y_pred_community)))
print('Coefficient of Determination (R-squared):', metrics.r2_score(y_test_community, y_pred_community))

"""### Using Police Data:"""

#Turn dataframe into a matrix
X_police = police_data.copy()
X_police = X_police.drop(['ViolentCrimesPerPop'], axis=1)
X_police = X_police.values

y_police = police_data['ViolentCrimesPerPop'].copy()
y_police = y_police.values


#Split the data
X_train_police, X_test_police, y_train_police, y_test_police = train_test_split(X_police, y_police, random_state=0, test_size=0.2)
#X_model_police, X_valid_police, y_model_police, y_valid_police = train_test_split(X_train_police, y_train_police, random_state=0, test_size=0.2)

print(f"All Data:        {len(X_police)} points")
print(f"Training data:   {len(X_train_police)} points")
print(f"Testing data:    {len(X_test_police)} points")

#Create regression object
regressor_police = LinearRegression()  

#Fit it to our training data
regressor_police.fit(X_train_police, y_train_police)

#Used fitted model to predict values
y_pred_police = regressor_police.predict(X_test_police)

#Error metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_police, y_pred_police))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test_police, y_pred_police))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_police, y_pred_police)))
print('Coefficient of Determination (R-squared):', metrics.r2_score(y_test_police, y_pred_police))

"""## Multiple Regression Model Using PCA (PCR)

### Using Community Data:
"""

#Note: DATA IS ALREADY NORMALIZED

#Determine number of components
pca_num = PCA().fit(X_community)

#Plotting the Cumulative Summation of the Explained Variance
plt.figure()
plt.plot(np.cumsum(pca_num.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)') #for each component
plt.title('Explained Variance')
plt.show()

#We will use 20 components

#Split the data
X_train_com_pca, X_test_com_pca, y_train_com_pca, y_test_com_pca = train_test_split(X_community, y_community, random_state=0, test_size=0.2)

#Create PCA object
pca_tf_com = PCA(n_components = 20)

#Transform the community data
X_train_com_pca = pca_tf_com.fit_transform(X_train_com_pca)
X_test_com_pca = pca_tf_com.transform(X_test_com_pca)

#Find the proportion of the data that is explained by the PCs
#We want ~90% of the variance explained (from above, 20 components)
pca_tf_com.explained_variance_ratio_

#Create regression object
regressor_community_pca = LinearRegression()  

#Fit it to our training data
regressor_community_pca.fit(X_train_com_pca, y_train_com_pca)

#Used fitted model to predict values
y_pred_com_pca = regressor_community_pca.predict(X_test_com_pca)

#Error metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_com_pca, y_pred_com_pca))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test_com_pca, y_pred_com_pca))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_com_pca, y_pred_com_pca)))
print('Coefficient of Determination (R-squared):', metrics.r2_score(y_test_com_pca, y_pred_com_pca))

"""### Using Police Data:"""

#Note: DATA IS ALREADY NORMALIZED

#Determine number of components
pca_num = PCA().fit(X_police)

#Plotting the Cumulative Summation of the Explained Variance
plt.figure()
plt.plot(np.cumsum(pca_num.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)') #for each component
plt.title('Explained Variance')
plt.show()

#We will use 10 components

#Turn dataframe into a matrix
X_police = police_data.copy()
X_police = X_police.drop(['ViolentCrimesPerPop'], axis=1)
X_police = X_police.values

y_police = police_data['ViolentCrimesPerPop'].copy()
y_police = y_police.values

#Split the data
X_train_pl_pca, X_test_pl_pca, y_train_pl_pca, y_test_pl_pca = train_test_split(X_police, y_police, random_state=0, test_size=0.2)

#Create PCA object
pca_tf_pl = PCA(n_components = 10)

#Transform the community data
X_train_pl_pca = pca_tf_pl.fit_transform(X_train_pl_pca)
X_test_pl_pca = pca_tf_pl.transform(X_test_pl_pca)

#Find the proportion of the data that is explained by the PCs
#We want ~90% of the variance explained (from above, 10 components)
pca_tf_pl.explained_variance_ratio_

#Create regression object
regressor_police_pca = LinearRegression()  

#Fit it to our training data
regressor_police_pca.fit(X_train_pl_pca, y_train_pl_pca)

#Used fitted model to predict values
y_pred_pl_pca = regressor_police_pca.predict(X_test_pl_pca)

#Error metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_pl_pca, y_pred_pl_pca))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test_pl_pca, y_pred_pl_pca))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_pl_pca, y_pred_pl_pca)))
print('Coefficient of Determination (R-squared):', metrics.r2_score(y_test_pl_pca, y_pred_pl_pca))

"""## Regularization

### Lasso Using Community Data:
"""

#Determine Coefficients

# Create some alpha values that we'll use to train several models
n_alphas = 200
alphas = np.logspace(-10, -1, n_alphas)

def determine_coefficients(alphas, model, X, y):
    """Determine the coefficients of a linear model (Lasso or Ridge) given the various alphas. 
    You should train a model for each value of alpha and store its coefficients to be returned.
    
    Args:
        alphas (iterable): The alphas to test out with the model
        model (sklearn.estimator Class): A type of linear model not instantiated
        X (iterable): The data to train on
        y (iterable): The labels to train on
        
    Returns:
        coefs (iterable): the coefficients extracted from the trained model. See model.coef_
    """
    
    coeff = []
    
    for i in alphas:
        clf = model(alpha=i, fit_intercept=False)
        model_fit = clf.fit(X, y) 
        c = model_fit.coef_ 
        coeff.append(c)
    return coeff

import warnings
warnings.filterwarnings("ignore")

lassoCoefs_com = determine_coefficients(alphas, Lasso, X_community, y_community)

ax = plt.gca()

ax.plot(alphas, lassoCoefs_com)
ax.set_xscale('log')
plt.xlabel('alpha')
plt.ylabel('Coefficient Value')
plt.title('Lasso coefficients as a \nfunction of the regularization weight')
plt.axis('tight')
plt.show()

#Split the data
X_train_community, X_test_community, y_train_community, y_test_community = train_test_split(X_community, y_community, random_state=0, test_size=0.2)

#Create, fit, and predict
alpha = 0.01
lassoreg_com = Lasso(alpha=alpha, max_iter=1e-5)
lassoreg_com.fit(X_train_community,y_train_community)
y_pred_lasso_com = lassoreg_com.predict(X_test_community)

#Error metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_community, y_pred_lasso_com))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test_community, y_pred_lasso_com))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_community, y_pred_lasso_com)))
print('Coefficient of Determination (R-squared):', metrics.r2_score(y_test_community, y_pred_lasso_com))

"""### Lasso Using Police Data:"""

#Determine coefficients
import warnings
warnings.filterwarnings("ignore")

lassoCoefs_pl = determine_coefficients(alphas, Lasso, X_police, y_police)

ax = plt.gca()

ax.plot(alphas, lassoCoefs_pl)
ax.set_xscale('log')
plt.xlabel('alpha')
plt.ylabel('Coefficient Value')
plt.title('Lasso coefficients as a \nfunction of the regularization weight')
plt.axis('tight')
plt.show()

#Split the data
X_train_pl, X_test_pl, y_train_pl, y_test_pl = train_test_split(X_police, y_police, random_state=0, test_size=0.2)

#Create, fit, and predict
alpha = 0.001
lassoreg_pl = Lasso(alpha=alpha, max_iter=1e5)
lassoreg_pl.fit(X_train_pl, y_train_pl)
y_pred_lasso_pl = lassoreg_pl.predict(X_test_pl)

#Error metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_pl, y_pred_lasso_pl))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test_pl, y_pred_lasso_pl))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_pl, y_pred_lasso_pl)))
print('Coefficient of Determination (R-squared):', metrics.r2_score(y_test_pl, y_pred_lasso_pl))

"""#Classification Analysis

###Create Dataset with Classifcation Targets
We take our community and police data sets and convert the target (Violent crime per capita) into discrete targets: Low (<0.33), Medium (<0.66), and High (<1). Note that our data is standardized.
"""

#Find max and min of normalized data
print(community_data['ViolentCrimesPerPop'].max())
print(community_data['ViolentCrimesPerPop'].min())

print(police_data['ViolentCrimesPerPop'].max())
print(police_data['ViolentCrimesPerPop'].min())

#Create function that will sort continuous targets into classes
def crime_class(x):
    if x <= 0.33:
        return "L"
    elif x <= 0.66:
        return "M"
    else: return "H"

#Make targets discrete
community_data_class = community_data.copy()
community_data_class['ViolentCrimeLevel'] = community_data_class['ViolentCrimesPerPop'].apply(crime_class)
community_data_class = community_data_class.drop(['ViolentCrimesPerPop'], axis=1)
community_data_class.head()

#Make targets discrete
police_data_class = police_data.copy()
police_data_class['ViolentCrimeLevel'] = police_data_class['ViolentCrimesPerPop'].apply(crime_class)
police_data_class = police_data_class.drop(['ViolentCrimesPerPop'], axis=1)
police_data_class.head()

"""##K-Nearest Neighbors

###Community Data:
"""

#Turn dataframe into a matrix
X_c_class = community_data_class.copy()
X_c_class = X_c_class.drop(['ViolentCrimeLevel'], axis=1)
X_c_class = X_c_class.values

y_c_class = community_data_class['ViolentCrimeLevel'].copy()
y_c_class = y_c_class.values

#Split the data into training and test data
X_train_c_knn, X_test_c_knn, y_train_c_knn, y_test_c_knn = train_test_split(X_c_class, y_c_class, random_state=0, test_size=0.2)

#Train a model on the training dataset then return the F-1 score on the test set   
#Initialize dictionary
f1_scores_k= dict()

ks = [1,3,5,7,10,15,20,30,40,50,70]
    
for i in ks:
    # Step 1 - Initialize model with parameters
    knn = KNeighborsClassifier(n_neighbors=i)
       
    # Step 2 - Fit the model data
    knn.fit(X_train_c_knn, y_train_c_knn)
        
    # Step 3 - Predict the test data
    class_predictions = knn.predict(X_test_c_knn)
        
    #Step 4 - Evaluate model using f1 score
    score = f1_score(y_test_c_knn, class_predictions, average="weighted")
        
    #Step 5 - Add key and value to dictionary
    f1_scores_k.update( {i : score} )
    
pd.Series(f1_scores_k).plot(kind="line")
plt.title("F1-Scores for various k values (Community Data)")
plt.xlabel("k")
plt.ylabel("F1-score")
plt.show()

"""We will use k=20 for our analysis:"""

bestK = 20
best_c_knn = KNeighborsClassifier(bestK)
best_c_knn.fit(X_train_c_knn, y_train_c_knn)
testPredictions = best_c_knn.predict(X_test_c_knn)

print("Confusion Matrix: \n")
print(confusion_matrix(y_test_c_knn, testPredictions))
print("\n\nClassification Report:\n")
print(classification_report(y_test_c_knn, testPredictions))

"""###Police Data:"""

#Turn dataframe into a matrix
X_p_class = police_data_class.copy()
X_p_class = X_p_class.drop(['ViolentCrimeLevel'], axis=1)
X_p_class = X_p_class.values

y_p_class = police_data_class['ViolentCrimeLevel'].copy()
y_p_class = y_p_class.values

#Split the data into training and test data
X_train_p_knn, X_test_p_knn, y_train_p_knn, y_test_p_knn = train_test_split(X_p_class, y_p_class, random_state=0, test_size=0.2)

#Train a model on the training dataset then return the F-1 score on the test set   
#Initialize dictionary
f1_scores_k= dict()

ks = [1,3,5,7,10,15,20,30,40,50,70]
    
for i in ks:
    # Step 1 - Initialize model with parameters
    knn = KNeighborsClassifier(n_neighbors=i)
       
    # Step 2 - Fit the model data
    knn.fit(X_train_p_knn, y_train_p_knn)
        
    # Step 3 - Predict the test data
    class_predictions = knn.predict(X_test_p_knn)
        
    #Step 4 - Evaluate model using f1 score
    score = f1_score(y_test_p_knn, class_predictions, average="weighted")
        
    #Step 5 - Add key and value to dictionary
    f1_scores_k.update( {i : score} )
    
pd.Series(f1_scores_k).plot(kind="line")
plt.title("F1-Scores for various k values (Police Data)")
plt.xlabel("k")
plt.ylabel("F1-score")
plt.show()

"""We will use k=10 for our analysis:"""

bestK = 10
best_p_knn = KNeighborsClassifier(bestK)
best_p_knn.fit(X_train_p_knn, y_train_p_knn)
testPredictions = best_p_knn.predict(X_test_p_knn)

print("Confusion Matrix: \n")
print(confusion_matrix(y_test_p_knn, testPredictions))
print("\n\nClassification Report:\n")
print(classification_report(y_test_p_knn, testPredictions))

"""#Neural Network Analysis

###Set Up:
"""

!pip install --upgrade tensorflow==2.0

import tensorflow as tf
import tensorflow.keras as keras
import numpy as np
from tensorflow.keras.layers import Dense, Dropout

np.random.seed(0)

#Split the data into training and test data
#We will do this both for regression and for classification

#Community - Continuous Targets
X_train_community, X_test_community, y_train_community, y_test_community = train_test_split(X_community, y_community, random_state=0, test_size=0.2)

#Community - Discrete Targets
X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_c_class, y_c_class, random_state=0, test_size=0.2)

#Police - Continuous Targets
X_train_police, X_test_police, y_train_police, y_test_police = train_test_split(X_police, y_police, random_state=0, test_size=0.2)

#Police - Discrete Targets
X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_p_class, y_p_class, random_state=0, test_size=0.2)

"""##Community Data

###Continuous Targets
"""

#Set up data for feed forward neural network
print(X_train_community.shape)
print(y_train_community.shape)

X_train_community = np.asarray(X_train_community)
y_train_community = np.asarray(y_train_community)

X_train_community = tf.convert_to_tensor(X_train_community, dtype=tf.float32)
y_train_community  = tf.convert_to_tensor(y_train_community, dtype=tf.float32)
X_test_community = tf.convert_to_tensor(X_test_community, dtype=tf.float32)
y_test_community  = tf.convert_to_tensor(y_test_community, dtype=tf.float32)

#Set up layers
layers = [
    Dense(10, input_shape=(100,), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((1))
]
model_community1 = keras.Sequential(layers)

#Train (fit) model and evaluate
model_community1.compile(optimizer='adam', loss='mae', metrics=["mae", "mse"])
model_community1.fit(X_train_community, y_train_community, epochs=500, verbose=0)
model_community1.summary()
model_community1.evaluate(X_test_community, y_test_community)

"""###Discrete Targets"""

#Set up data for feed forward neural network
print(X_train_c.shape)
print(y_train_c.shape)
print(X_test_c.shape)
print(y_test_c.shape)

X_train_c = np.asarray(X_train_c)
y_train_c = np.asarray(y_train_c)

X_train_c = tf.convert_to_tensor(X_train_c, dtype=tf.float32)
y_train_c  = tf.convert_to_tensor(y_train_c, dtype=tf.string)
X_test_c = tf.convert_to_tensor(X_test_c, dtype=tf.float32)
y_test_c  = tf.convert_to_tensor(y_test_c, dtype=tf.string)

from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils

#Encode class values as integers
encoder = LabelEncoder()
encoder.fit(y_train_c)
encoded_Y = encoder.transform(y_train_c)
#Convert integers to dummy variables (i.e. one hot encoded)
y_train_c = np_utils.to_categorical(encoded_Y)

#Encode class values as integers
encoder2 = LabelEncoder()
encoder2.fit(y_test_c)
encoded_Y = encoder2.transform(y_test_c)
#Convert integers to dummy variables (i.e. one hot encoded)
y_test_c = np_utils.to_categorical(encoded_Y)

from keras.optimizers import SGD
#Set up layers
layers = [
    Dense(10, input_shape=(100,), activation = 'tanh'),
    Dropout(0.05),
    Dense(3, activation='softmax')
]
model_community2 = keras.Sequential(layers)

#Train (fit) model and evaluate
model_community2.compile(optimizer='adam', loss='mae', metrics=["accuracy"])
model_community2.fit(X_train_c, y_train_c, epochs=500, verbose=0)
model_community2.summary()
model_community2.evaluate(X_test_c, y_test_c)

"""##Police Data

###Continuous Targets
"""

#Set up data for feed forward neural network
print(X_train_police.shape)
print(y_train_police.shape)

X_train_police = np.asarray(X_train_police)
y_train_police = np.asarray(y_train_police)

X_train_police = tf.convert_to_tensor(X_train_police, dtype=tf.float32)
y_train_police  = tf.convert_to_tensor(y_train_police, dtype=tf.float32)
X_test_police = tf.convert_to_tensor(X_test_police, dtype=tf.float32)
y_test_police  = tf.convert_to_tensor(y_test_police, dtype=tf.float32)

#Set up layers
layers = [
    Dense(10, input_shape=(22,), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dense((1), activation = 'tanh')
]
model = keras.Sequential(layers)

#Train (fit) model and evaluate
model.compile(optimizer='adam', loss='mae', metrics=["mae", "mse"])
model.fit(X_train_police, y_train_police, epochs=500, verbose=0)
model.summary()
model.evaluate(X_test_police, y_test_police)

"""###Discrete Targets"""

#Set up data for feed forward neural network
print(X_train_p.shape)
print(y_train_p.shape)
print(X_test_p.shape)
print(y_test_p.shape)

X_train_p = np.asarray(X_train_p)
y_train_p = np.asarray(y_train_p)

X_train_p = tf.convert_to_tensor(X_train_p, dtype=tf.float32)
y_train_p  = tf.convert_to_tensor(y_train_p, dtype=tf.string)
X_test_p = tf.convert_to_tensor(X_test_p, dtype=tf.float32)
y_test_p  = tf.convert_to_tensor(y_test_p, dtype=tf.string)

from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils

#Encode class values as integers
encoder = LabelEncoder()
encoder.fit(y_train_p)
encoded_Y = encoder.transform(y_train_p)
#Convert integers to dummy variables (i.e. one hot encoded)
y_train_p = np_utils.to_categorical(encoded_Y)

#Encode class values as integers
encoder2 = LabelEncoder()
encoder2.fit(y_test_p)
encoded_Y = encoder2.transform(y_test_p)
#Convert integers to dummy variables (i.e. one hot encoded)
y_test_p = np_utils.to_categorical(encoded_Y)

#Set up layers
layers = [
    Dense(10, input_shape=(22,), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense(3, activation='softmax')
]
model_police2 = keras.Sequential(layers)

#Train (fit) model and evaluate
model_police2.compile(optimizer='adam', loss='mae', metrics=["accuracy"])
model_police2.fit(X_train_p, y_train_p, epochs=500, verbose=0)
model_police2.summary()

model_police2.evaluate(X_test_p, y_test_p)

"""##Lime Library

This don't work homies
"""

!pip install lime
import lime
import lime.lime_tabular
import sklearn.ensemble

#rf = sklearn.ensemble.RandomForestRegressor(n_estimators=1000)
#train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(X_community, y_community, train_size=0.80)
#rf.fit(train, labels_train)
#print('Random Forest MSError', np.mean((rf.predict(test) - labels_test) ** 2))

#X_community = community_data.copy()
#X_community = X_community.drop(['ViolentCrimesPerPop'], axis=1)
#train=train.value
#train

#explainer = lime.lime_tabular.LimeTabularExplainer(train, feature_names=list(X_community.columns.values), class_names=['ViolentCrimesPerPop'], mode='regression')

"""#Feature Selection Procedures

###Univariate Selection
"""

#Read in data again because things got WONKY
from google.colab import files
uploaded = files.upload()

import io
raw_data = pd.read_csv(io.BytesIO(uploaded['communities.csv']))

#Replace '?' with NA values
raw_data = raw_data.replace('?', np.NaN)

#Remove fold column
raw_data = raw_data.drop(columns=["fold"])

#Remove the state, county, community, and community name columns
#Create dataframe of community statistics (1993 observations)
community_data = raw_data.copy()
community_data = community_data.drop(columns=["state", "county", "community", "communityname"])

community_data = community_data[[c for c in community_data if community_data[c].isnull().sum() < 1000]]
community_data.dropna(inplace=True)
#Create dataframe of crime/police statistics (319 observations)
police_data = raw_data[['LemasSwornFT',	'LemasSwFTPerPop',	'LemasSwFTFieldOps',	'LemasSwFTFieldPerPop',	'LemasTotalReq',	'LemasTotReqPerPop',	'PolicReqPerOffic',	'PolicPerPop',	'RacialMatchCommPol',	'PctPolicWhite',	'PctPolicBlack',	'PctPolicHisp',	'PctPolicAsian',	'PctPolicMinor',	'OfficAssgnDrugUnits',	'NumKindsDrugsSeiz',	'PolicAveOTWorked',	'PolicCars',	'PolicOperBudg',	'LemasPctPolicOnPatr',	'LemasGangUnitDeploy',	'PolicBudgPerPop', 'ViolentCrimesPerPop']].copy()
police_data.dropna(inplace=True)

#Define crime classes
def crime_class(x):
    if x <= 0.33:
        return "L"
    elif x <= 0.66:
        return "M"
    else: return "H"

#Set up data
community_data_class = community_data.copy()
community_data_class['ViolentCrimeLevel'] = community_data_class['ViolentCrimesPerPop'].apply(crime_class)
community_data_class = community_data_class.drop(['ViolentCrimesPerPop'], axis=1)
community_data_class.head()

X_c_class = community_data_class.copy()
X_c_class = X_c_class.drop(['ViolentCrimeLevel'], axis=1)

y_c_class = community_data_class['ViolentCrimeLevel'].copy()

police_data_class = police_data.copy()
police_data_class['ViolentCrimeLevel'] = police_data_class['ViolentCrimesPerPop'].apply(crime_class)
police_data_class = police_data_class.drop(['ViolentCrimesPerPop'], axis=1)
police_data_class.head()

#Split the data
X_train_community, X_test_community, y_train_community, y_test_community = train_test_split(X_c_class, y_c_class, random_state=0, test_size=0.2)

#apply SelectKBest class to extract top 5 best features
bestfeatures = SelectKBest(score_func=chi2, k=5)
fit = bestfeatures.fit(X_train_community, y_train_community)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X_train_community.columns)
#concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Feature','Score']  #naming the dataframe columns
print(featureScores.nlargest(5,'Score'))  #print 5 best features

"""##Feature Importance"""

from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

model = ExtraTreesClassifier()
model.fit(X_train_community,y_train_community)
print(model.feature_importances_) 

#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=X_train_community.columns)
feat_importances.nlargest(10).plot(kind='barh', color='steelblue', title = "Feature Importances")
plt.show()

"""##Feature Selector Class"""

#Note: make sure that once Feature Selector is installed, you comment out the below line
#An error will result otherwise
#!pip install feature_selector
import feature_selector 
from feature_selector import FeatureSelector
import matplotlib.pyplot as plt
import seaborn as sns

fs = FeatureSelector(data = X_train_community, labels = y_train_community)

fs.identify_collinear(correlation_threshold = 0.8)
collinear_features = fs.ops['collinear']
fs.record_collinear.head()

# Pass in the appropriate parameters
fs.identify_zero_importance(task = 'classification', 
                            eval_metric = 'auc', 
                            n_iterations = 10, 
                             early_stopping = True)
# list of zero importance features
zero_importance_features = fs.ops['zero_importance']

# plot the feature importances
fs.plot_feature_importances(threshold = 0.90, plot_n = 10)

"""#Building the Model with Limited Features

###Discrete Targets
"""

limited_data = community_data_class[["PctIlleg", "PctKids2Par", "racePctWhite", "PctPopUnderPov", "PctVacantBoarded", "ViolentCrimeLevel"]]
limited_data.head()

#Create X and y datasets with limited data
X_limited = limited_data.copy()
X_limited = X_limited.drop(['ViolentCrimeLevel'], axis=1)
y_limited = limited_data['ViolentCrimeLevel'].copy()

X_train_limited, X_test_limited, y_train_limited, y_test_limited = train_test_split(X_limited, y_limited, random_state=0, test_size=0.2)
X_test_limited.head()

#Set up data for model building
import tensorflow as tf
import tensorflow.keras as keras
import numpy as np
from tensorflow.keras.layers import Dense, Dropout

print(X_train_limited.shape)
print(y_train_limited.shape)
print(X_test_limited.shape)
print(y_test_limited.shape)

X_train_limited = np.asarray(X_train_limited)
y_train_limited = np.asarray(y_train_limited)
X_test_limited = np.asarray(X_test_limited)
y_test_limited = np.asarray(y_test_limited)

X_train_limited = tf.convert_to_tensor(X_train_limited, dtype=tf.float32)
y_train_limited  = tf.convert_to_tensor(y_train_limited, dtype=tf.string)
X_test_limited = tf.convert_to_tensor(X_test_limited, dtype=tf.float32)
y_test_limited  = tf.convert_to_tensor(y_test_limited, dtype=tf.string)

from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils

#Encode class values as integers
encoder = LabelEncoder()
encoder.fit(y_train_limited)
encoded_Y = encoder.transform(y_train_limited)
#Convert integers to dummy variables (i.e. one hot encoded)
y_train_limited = np_utils.to_categorical(encoded_Y)

#Encode class values as integers
encoder2 = LabelEncoder()
encoder2.fit(y_test_limited)
encoded_Y = encoder2.transform(y_test_limited)
#Convert integers to dummy variables (i.e. one hot encoded)
y_test_limited = np_utils.to_categorical(encoded_Y)

layers = [
    Dense(10, input_shape=(5,), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dense((3), activation = 'softmax')
]
model_community = keras.Sequential(layers)

model_community.compile(optimizer='adam', loss='mae', metrics=["accuracy"])
model_community.fit(X_train_limited, y_train_limited, epochs=500, verbose=0)
model_community.summary()
model_community.evaluate(X_test_limited, y_test_limited)

"""###Continuous Targets"""

#Create X and y datasets with limited data
limited_data = community_data[["PctIlleg", "PctKids2Par", "racePctWhite", "PctPopUnderPov", "PctVacantBoarded", "ViolentCrimesPerPop"]]
X_limited = limited_data.copy()
X_limited = X_limited.drop(['ViolentCrimesPerPop'], axis=1)
y_limited = limited_data['ViolentCrimesPerPop'].copy()

X_train_limited, X_test_limited, y_train_limited, y_test_limited = train_test_split(X_limited, y_limited, random_state=0, test_size=0.2)
X_test_limited.head()

#Set up data for model building
import tensorflow as tf
import tensorflow.keras as keras
import numpy as np
from tensorflow.keras.layers import Dense, Dropout

print(X_train_limited.shape)
print(y_train_limited.shape)
print(X_test_limited.shape)
print(y_test_limited.shape)

X_train_limited = np.asarray(X_train_limited)
y_train_limited = np.asarray(y_train_limited)
X_test_limited = np.asarray(X_test_limited)
y_test_limited = np.asarray(y_test_limited)

X_train_limited = tf.convert_to_tensor(X_train_limited, dtype=tf.float32)
y_train_limited  = tf.convert_to_tensor(y_train_limited, dtype=tf.float32)
X_test_limited = tf.convert_to_tensor(X_test_limited, dtype=tf.float32)
y_test_limited  = tf.convert_to_tensor(y_test_limited, dtype=tf.float32)

layers = [
    Dense(10, input_shape=(5,), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dense(1)
]
model_community = keras.Sequential(layers)

model_community.compile(optimizer='adam', loss='mae', metrics=["mae", "mse"])
model_community.fit(X_train_limited, y_train_limited, epochs=500, verbose=0)
model_community.summary()
model_community.evaluate(X_test_limited, y_test_limited)

"""#Investigating relationship with poverty"""

#Correlations of limited data
corr = limited_data.corr()
sns.heatmap(corr, annot=True, cmap=plt.cm.Blues)
plt.show()

"""### Continuous Targets"""

#Predict using poverty
#Create X and y datasets with limited data
limited_data = community_data[["PctPopUnderPov", "ViolentCrimesPerPop"]]
X_limited = limited_data.copy()
X_limited = X_limited.drop(['ViolentCrimesPerPop'], axis=1)
y_limited = limited_data['ViolentCrimesPerPop'].copy()

X_train_limited, X_test_limited, y_train_limited, y_test_limited = train_test_split(X_limited, y_limited, random_state=0, test_size=0.2)
X_test_limited.head()

#Set up data for model building
import tensorflow as tf
import tensorflow.keras as keras
import numpy as np
from tensorflow.keras.layers import Dense, Dropout

print(X_train_limited.shape)
print(y_train_limited.shape)
print(X_test_limited.shape)
print(y_test_limited.shape)

X_train_limited = np.asarray(X_train_limited)
y_train_limited = np.asarray(y_train_limited)
X_test_limited = np.asarray(X_test_limited)
y_test_limited = np.asarray(y_test_limited)

X_train_limited = tf.convert_to_tensor(X_train_limited, dtype=tf.float32)
y_train_limited  = tf.convert_to_tensor(y_train_limited, dtype=tf.float32)
X_test_limited = tf.convert_to_tensor(X_test_limited, dtype=tf.float32)
y_test_limited  = tf.convert_to_tensor(y_test_limited, dtype=tf.float32)

layers = [
    Dense(10, input_shape=(1,), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense(1, activation = 'tanh')
]
model_community = keras.Sequential(layers)

model_community.compile(optimizer='adam', loss='mae', metrics=["mae", "mse"])
model_community.fit(X_train_limited, y_train_limited, epochs=500, verbose=0)
model_community.summary()
model_community.evaluate(X_test_limited, y_test_limited)

#Try using linear regression as well
limited_data = community_data[["PctPopUnderPov", "ViolentCrimesPerPop"]]
X_limited = limited_data.copy()
X_limited = X_limited.drop(['ViolentCrimesPerPop'], axis=1)
y_limited = limited_data['ViolentCrimesPerPop'].copy()

X_train_limited, X_test_limited, y_train_limited, y_test_limited = train_test_split(X_limited, y_limited, random_state=0, test_size=0.2)

#Create regression object
regressor_limited = LinearRegression()  

#Fit it to our training data
regressor_limited.fit(X_train_limited, y_train_limited)

#Used fitted model to predict values
y_pred_limited = regressor_limited.predict(X_test_limited)

#Error metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_limited, y_pred_limited))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test_limited, y_pred_limited))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_limited, y_pred_limited)))
print('Coefficient of Determination (R-squared):', metrics.r2_score(y_test_limited, y_pred_limited))

"""###Discrete Targets"""

limited_data = community_data_class[["PctPopUnderPov", "ViolentCrimeLevel"]]
limited_data.head()
#Create X and y datasets with limited data
X_limited = limited_data.copy()
X_limited = X_limited.drop(['ViolentCrimeLevel'], axis=1)
y_limited = limited_data['ViolentCrimeLevel'].copy()

X_train_limited, X_test_limited, y_train_limited, y_test_limited = train_test_split(X_limited, y_limited, random_state=0, test_size=0.2)
X_test_limited.head()

#Set up data for model building
import tensorflow as tf
import tensorflow.keras as keras
import numpy as np
from tensorflow.keras.layers import Dense, Dropout

print(X_train_limited.shape)
print(y_train_limited.shape)
print(X_test_limited.shape)
print(y_test_limited.shape)

X_train_limited = np.asarray(X_train_limited)
y_train_limited = np.asarray(y_train_limited)
X_test_limited = np.asarray(X_test_limited)
y_test_limited = np.asarray(y_test_limited)

X_train_limited = tf.convert_to_tensor(X_train_limited, dtype=tf.float32)
y_train_limited  = tf.convert_to_tensor(y_train_limited, dtype=tf.string)
X_test_limited = tf.convert_to_tensor(X_test_limited, dtype=tf.float32)
y_test_limited  = tf.convert_to_tensor(y_test_limited, dtype=tf.string)

from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils

#Encode class values as integers
encoder = LabelEncoder()
encoder.fit(y_train_limited)
encoded_Y = encoder.transform(y_train_limited)
#Convert integers to dummy variables (i.e. one hot encoded)
y_train_limited = np_utils.to_categorical(encoded_Y)

#Encode class values as integers
encoder2 = LabelEncoder()
encoder2.fit(y_test_limited)
encoded_Y = encoder2.transform(y_test_limited)
#Convert integers to dummy variables (i.e. one hot encoded)
y_test_limited = np_utils.to_categorical(encoded_Y)

layers = [
    Dense(10, input_shape=(1,), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dropout(0.05),
    Dense((10), activation = 'tanh'),
    Dense((3), activation = 'softmax')
]
model_community = keras.Sequential(layers)

model_community.compile(optimizer='adam', loss='mae', metrics=["accuracy"])
model_community.fit(X_train_limited, y_train_limited, epochs=500, verbose=0)
model_community.summary()
model_community.evaluate(X_test_limited, y_test_limited)

